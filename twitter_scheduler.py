#!/usr/bin/env python3
"""
Twitter çˆ¬è™«ç®€åŒ–è°ƒåº¦å™¨
æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼Œè‡ªåŠ¨S3ä¸Šä¼ 
"""

import schedule
import time
import subprocess
import sys
import os
from datetime import datetime
import json
from pathlib import Path

# ========== é…ç½®åŒºåŸŸ ==========
CONFIG = {
    # çˆ¬è™«è„šæœ¬æ–‡ä»¶å
    "auto_crawler": "twitter_crawler.py",  # è‡ªåŠ¨çˆ¬è™«è„šæœ¬
    "s3_uploader": "s3_uploader.py",  # S3ä¸Šä¼ è„šæœ¬

    # å®šæ—¶è¿è¡Œé…ç½®
    "schedule": {
        "hourly_run": True,  # æ¯å°æ—¶è¿è¡Œä¸€æ¬¡
        "daily_limit": 24    # æ¯æ—¥æœ€å¤§è¿è¡Œæ¬¡æ•°ï¼ˆæ¯å°æ—¶ä¸€æ¬¡ï¼‰
    },

    # S3ä¸Šä¼ é…ç½®
    "s3": {
        "enabled": True,  # æ˜¯å¦å¯ç”¨S3ä¸Šä¼ 
        "auto_upload": True,  # çˆ¬è™«æˆåŠŸåè‡ªåŠ¨ä¸Šä¼ 
        "timeout": 300  # S3ä¸Šä¼ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    }
}


class SimpleCrawlerScheduler:
    def __init__(self):
        self.config = CONFIG
        self.stats_file = Path("scheduler_stats.json")
        self.daily_run_count = 0
        self.last_run_date = None
        self.run_times = []  # è®°å½•è¿è¡Œæ—¶é—´
        self.load_stats()

    def load_stats(self):
        """åŠ è½½è¿è¡Œç»Ÿè®¡"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    stats = json.load(f)
                    self.daily_run_count = stats.get('daily_run_count', 0)

                    last_date = stats.get('last_run_date')
                    if last_date:
                        self.last_run_date = datetime.strptime(last_date, '%Y-%m-%d').date()
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    def save_stats(self):
        """ä¿å­˜è¿è¡Œç»Ÿè®¡"""
        try:
            stats = {
                'daily_run_count': self.daily_run_count,
                'last_run_date': datetime.now().date().isoformat(),
                'last_update': datetime.now().isoformat(),
                'run_history': self.run_times[-10:]  # ä¿å­˜æœ€è¿‘10æ¬¡è¿è¡Œè®°å½•
            }
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    def check_daily_limit(self):
        """æ£€æŸ¥æ¯æ—¥è¿è¡Œæ¬¡æ•°é™åˆ¶"""
        current_date = datetime.now().date()

        # å¦‚æœæ˜¯æ–°çš„ä¸€å¤©ï¼Œé‡ç½®è®¡æ•°å™¨
        if self.last_run_date != current_date:
            self.daily_run_count = 0
            self.last_run_date = current_date

        return self.daily_run_count < self.config['schedule']['daily_limit']

    def upload_to_s3(self):
        """æ‰§è¡ŒS3ä¸Šä¼ """
        if not self.config['s3']['enabled']:
            print("ğŸ“¤ S3ä¸Šä¼ å·²ç¦ç”¨")
            return True

        print("\nğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°S3...")
        try:
            # æ£€æŸ¥S3ä¸Šä¼ è„šæœ¬æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.config['s3_uploader']):
                print(f"âŒ S3ä¸Šä¼ è„šæœ¬ä¸å­˜åœ¨: {self.config['s3_uploader']}")
                return False

            s3_process = subprocess.run(
                [sys.executable, self.config['s3_uploader']],
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=self.config['s3']['timeout']
            )

            if s3_process.returncode == 0:
                print("âœ… S3ä¸Šä¼ å®Œæˆ!")
                # æ˜¾ç¤ºä¸Šä¼ è¾“å‡º
                for line in s3_process.stdout.split('\n'):
                    if line.strip():
                        print(f"  > {line}")
                return True
            else:
                print("âŒ S3ä¸Šä¼ å¤±è´¥")
                if s3_process.stderr:
                    print(f"é”™è¯¯ä¿¡æ¯: {s3_process.stderr}")
                if s3_process.stdout:
                    print(f"è¾“å‡ºä¿¡æ¯: {s3_process.stdout}")
                return False

        except subprocess.TimeoutExpired:
            print(f"âŒ S3ä¸Šä¼ è¶…æ—¶ï¼ˆ{self.config['s3']['timeout']}ç§’ï¼‰")
            return False
        except Exception as e:
            print(f"âŒ è¿è¡ŒS3ä¸Šä¼ æ—¶å‡ºé”™: {e}")
            return False

    def run_crawler(self):
        """è¿è¡Œçˆ¬è™«"""
        if not self.check_daily_limit():
            print(f"âš ï¸ å·²è¾¾åˆ°æ¯æ—¥è¿è¡Œé™åˆ¶ ({self.config['schedule']['daily_limit']} æ¬¡)")
            return False

        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        print(f"\n{'=' * 60}")
        print(f"ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™« - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Š ä»Šæ—¥ç¬¬ {self.daily_run_count + 1} æ¬¡è¿è¡Œ")

        # è¿è¡Œçˆ¬è™«
        try:
            print(f"\nğŸ“ æ‰§è¡Œå‘½ä»¤: python {self.config['auto_crawler']}")

            # ä½¿ç”¨ subprocess è¿è¡Œçˆ¬è™«
            process = subprocess.Popen(
                [sys.executable, self.config['auto_crawler']],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                encoding='utf-8',
                errors='ignore',
                bufsize=1
            )

            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            output_lines = []
            try:
                for line in process.stdout:
                    print(f"  > {line.rstrip()}")
                    output_lines.append(line.rstrip())
            except Exception as e:
                print(f"  > [è¯»å–è¾“å‡ºæ—¶å‡ºé”™: {e}]")

            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()

            # è®°å½•è¿è¡Œæ—¶é—´
            end_time = time.time()
            duration = end_time - start_time
            self.run_times.append({
                'time': datetime.now().isoformat(),
                'duration': duration
            })

            # åªä¿ç•™æœ€è¿‘10æ¬¡è®°å½•
            if len(self.run_times) > 10:
                self.run_times.pop(0)

            # æ˜¾ç¤ºè¿è¡Œæ—¶é—´ç»Ÿè®¡
            print(f"\nâ±ï¸ è¿è¡Œæ—¶é—´ç»Ÿè®¡:")
            print(f"  - æœ¬æ¬¡è¿è¡Œ: {duration:.1f} ç§’ ({duration / 60:.1f} åˆ†é’Ÿ)")

            if len(self.run_times) > 1:
                avg_time = sum(r['duration'] for r in self.run_times) / len(self.run_times)
                print(f"  - å¹³å‡è¿è¡Œæ—¶é—´: {avg_time:.1f} ç§’ ({avg_time / 60:.1f} åˆ†é’Ÿ)")
                print(f"  - æœ€æ…¢è¿è¡Œ: {max(r['duration'] for r in self.run_times):.1f} ç§’")

            if return_code == 0:
                print("\nâœ… çˆ¬è™«è¿è¡ŒæˆåŠŸ!")
                self.daily_run_count += 1
                self.save_stats()

                # è‡ªåŠ¨è¿è¡ŒS3ä¸Šä¼ 
                if self.config['s3']['auto_upload']:
                    s3_success = self.upload_to_s3()
                    if not s3_success:
                        print("âš ï¸  æ³¨æ„: çˆ¬è™«æˆåŠŸä½†S3ä¸Šä¼ å¤±è´¥")

                return True
            else:
                print(f"\nâŒ çˆ¬è™«è¿è¡Œå¤±è´¥ï¼Œè¿”å›ç ï¼š{return_code}")
                return False

        except KeyboardInterrupt:
            print("\nâš ï¸  çˆ¬è™«è¢«æ‰‹åŠ¨ä¸­æ–­")
            process.terminate()
            raise
        except Exception as e:
            print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
            return False
        finally:
            print(f"{'=' * 60}\n")

    def job_wrapper(self):
        """ä½œä¸šåŒ…è£…å™¨ - ç”¨äºscheduleè°ƒç”¨"""
        try:
            self.run_crawler()
        except Exception as e:
            print(f"è°ƒåº¦ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("""
    ğŸ¤–  Twitter çˆ¬è™«ç®€åŒ–è°ƒåº¦å™¨
    ===========================
    æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼Œè‡ªåŠ¨ä¸Šä¼ S3
    """)

    # åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
    scheduler = SimpleCrawlerScheduler()

    # æ£€æŸ¥S3ä¸Šä¼ è„šæœ¬
    if scheduler.config['s3']['enabled']:
        if os.path.exists(scheduler.config['s3_uploader']):
            print(f"ğŸ“¤ S3ä¸Šä¼ : å·²å¯ç”¨ ({scheduler.config['s3_uploader']})")
        else:
            print(f"âš ï¸  S3ä¸Šä¼ è„šæœ¬ä¸å­˜åœ¨: {scheduler.config['s3_uploader']}")
            choice = input("æ˜¯å¦ç»§ç»­è¿è¡Œä½†ç¦ç”¨S3ä¸Šä¼ ï¼Ÿ(y/n): ")
            if choice.lower() == 'y':
                scheduler.config['s3']['enabled'] = False
                print("ğŸ“¤ S3ä¸Šä¼ å·²ç¦ç”¨")
            else:
                return
    else:
        print("ğŸ“¤ S3ä¸Šä¼ : å·²ç¦ç”¨")

    # è®¾ç½®å®šæ—¶ä»»åŠ¡ - æ¯å°æ—¶è¿è¡Œä¸€æ¬¡
    print("\nğŸ“… è®¾ç½®å®šæ—¶ä»»åŠ¡...")
    schedule.every().hour.do(scheduler.job_wrapper)
    print("  â° æ¯å°æ—¶è¿è¡Œä¸€æ¬¡")

    print(f"\nğŸ“Š æ¯æ—¥è¿è¡Œé™åˆ¶: {scheduler.config['schedule']['daily_limit']} æ¬¡")
    print("\nâœ… è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")

    # ç«‹å³è¿è¡Œä¸€æ¬¡
    print("\nğŸš€ ç«‹å³è¿è¡Œç¬¬ä¸€æ¬¡çˆ¬è™«...")
    scheduler.run_crawler()

    # ä¸»å¾ªç¯
    try:
        while True:
            schedule.run_pending()

            # æ˜¾ç¤ºçŠ¶æ€
            next_run = schedule.next_run()
            if next_run:
                current_time = datetime.now()
                if current_time.second == 0:  # æ¯åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡æ˜¾ç¤º
                    print(f"\râ³ ç­‰å¾…ä¸­... å½“å‰æ—¶é—´: {current_time.strftime('%H:%M:%S')} | "
                          f"ä¸‹æ¬¡è¿è¡Œ: {next_run.strftime('%H:%M:%S')} | "
                          f"ä»Šæ—¥å·²è¿è¡Œ: {scheduler.daily_run_count} æ¬¡", end='', flush=True)

            time.sleep(1)

    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ è°ƒåº¦å™¨å·²åœæ­¢")
        print(f"ğŸ“Š ä»Šæ—¥å…±è¿è¡Œ {scheduler.daily_run_count} æ¬¡")
        scheduler.save_stats()


if __name__ == "__main__":
    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•è¿è¡Œ
    try:
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        if hasattr(sys, 'frozen'):
            script_dir = os.path.dirname(sys.executable)
        else:
            script_dir = os.path.dirname(os.path.abspath(__file__))

        os.chdir(script_dir)
    except:
        pass

    main()